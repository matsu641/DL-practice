{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matsu641/DL-practice/blob/main/lecture03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第3回講義 演習  \n",
        "\n",
        "今回は，深層モデルやそのライブラリは用いず，多層パーセプトロンを実装します．\n",
        "\n"
      ],
      "metadata": {
        "id": "MnUTObth9EXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目次\n",
        "\n",
        "1. [【課題 1】多層パーセプトロンの実装と学習(XOR)](#scrollTo=GGnLlc7ACgxf)\n",
        "\n",
        "    1.1. [活性化関数とその微分](#scrollTo=Kxzqey199-33)\n",
        "    \n",
        "    1.2. [データセットの設定と重みの定義](#scrollTo=9X-jAIqoCWsp)\n",
        "    \n",
        "    1.3. [train関数とvalid関数](#scrollTo=ZcG-GIvyDYXe)\n",
        "    \n",
        "    1.4. [学習](#scrollTo=Ep9LqYJtPl_s)\n",
        "\n",
        "1. [【課題 2】多層パーセプトロンの実装と学習(MNIST)](#scrollTo=fdEpBD--P8fD)\n",
        "\n",
        "    2.1. [ソフトマックス関数](#scrollTo=UDUGHs8TfXH2)\n",
        "    \n",
        "    2.2. [データセットの設定](#scrollTo=vTArTuMYgYDk)\n",
        "    \n",
        "    2.3. [全結合層の定義](#scrollTo=UQ75UXddhar_)\n",
        "    \n",
        "    2.4. [train関数とvalid関数](#scrollTo=mK7lR2Q-lc5K)\n",
        "    \n",
        "    2.5. [学習](#scrollTo=n_O-NCslmW3p)\n",
        "\n",
        "    2.6. [Tips:実験の可視化](#scrollTo=1fkAlZBNJpyM)\n",
        "\n",
        "1. [【課題 3】数値微分（勾配チェック）](#scrollTo=WA98nAv1mxWu)\n",
        "\n",
        "    3.1. [1変数の場合](#scrollTo=cTFnh6oxofw2)\n",
        "\n",
        "    3.2. [多変数の場合(MLP)](#scrollTo=wCqJgtmipLrA)\n"
      ],
      "metadata": {
        "id": "Jt7ibMdOYDDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(34)"
      ],
      "metadata": {
        "id": "DgWU0L0D9Mp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.【課題 1】多層パーセプトロンの実装と学習(XOR)  "
      ],
      "metadata": {
        "id": "GGnLlc7ACgxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. 活性化関数とその微分\n",
        "まずは活性化関数の定義と，勾配の計算に利用する導関数を定義していきます．ここではsigmoid関数，ReLU関数，tanh関数を実装していきます．\n",
        "\n",
        "sigmoid関数は二値分類の出力層，ReLU関数とtanh関数は隠れ層の活性化関数として用いられることが多いですが，近年では勾配消失問題の対策としてReLU関数を利用するのが一般的です．  "
      ],
      "metadata": {
        "id": "Kxzqey199-33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sigmoid関数**\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma(x) = \\frac{1}{1+\\text{exp}(-x)} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma'(x) = \\sigma(x)(1-\\sigma(x)) \\tag{2}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "mxl16kJQkLEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    # 単純な実装\n",
        "    # return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # expのoverflow対策を施した実装\n",
        "    # x >=0 のとき sigmoid(x) = 1 / (1 + exp(-x))\n",
        "    # x < 0 のとき sigmoid(x) = exp(x) / (1 + exp(x))\n",
        "    return np.exp(np.minimum(x, 0)) / (1 + np.exp(- np.abs(x)))\n",
        "\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "0QPWWfCj-v_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReLU関数**\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{ReLU}(x) = \\text{max}(0, x) \\tag{3}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{ReLU}'(x) =  \\begin{cases}\n",
        "    1 \\quad \\text{if} \\quad x > 0 \\tag{4} \\\\\n",
        "    0 \\quad \\text{otherwise}\n",
        "    \\end{cases}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "4BxK6w5L_iGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def deriv_relu(x):\n",
        "    return (x > 0).astype(x.dtype)"
      ],
      "metadata": {
        "id": "Pxa4PUmbBq3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tanh関数**\n",
        "\\begin{equation}\n",
        "    \\text{tanh}(x) = \\frac{\\text{exp}(2x)-1}{\\text{exp}(2x)+1} \\tag{5}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\text{tanh}'(x) = 1 - \\text{tanh}^2(x) \\tag{6}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "if9Fmb3UByDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def deriv_tanh(x):\n",
        "    return 1 - tanh(x) ** 2"
      ],
      "metadata": {
        "id": "Y3MBPh5rCPrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. データセットの設定と重みの定義  \n",
        "次にMLPを学習するためのデータセットと，パラメータの初期化を行います．\n",
        "\n",
        "まずはデータセットを作成します．\n",
        "\n",
        "データセットは非線形問題として知られるXOR問題を用います．講義でも扱いましたが，XOR問題は2つの入力$(x_1, x_2), \\quad x_1, x_2 \\in \\{0, 1\\}$を与え，2つの値が同じときは0，異なるときは1を割り当てます．これは二次元空間に描画した時に0に割り当てられる点と1に割り当てられる点を1つの直線で分類することができないため，非線形問題となっています．"
      ],
      "metadata": {
        "id": "9X-jAIqoCWsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XORデータセット\n",
        "x_train_xor = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
        "t_train_xor = np.array([[1], [1], [0], [0]])\n",
        "x_valid_xor, t_valid_xor = x_train_xor, t_train_xor\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.hlines([0], xmin=-1, xmax=2, color=\"black\", alpha=0.7)\n",
        "plt.vlines([0], ymin=-1, ymax=2, color=\"black\", alpha=0.7)\n",
        "plt.scatter(x_train_xor[0:2, 0], x_train_xor[0:2, 1], color=\"red\", label=\"1\")\n",
        "plt.scatter(x_train_xor[2:, 0], x_train_xor[2:, 1], color=\"blue\", label=\"0\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.xlim([-0.5, 1.5])\n",
        "plt.ylim([-0.5, 1.5])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "7AX0WH1j6x6f",
        "outputId": "3f057ff9-d0f0-48f2-d6d1-5e654e4dc8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にパラメータを初期化します．重みは一様分布からのサンプリング，バイアスは0で初期化を行います．"
      ],
      "metadata": {
        "id": "ZO_FNY4N7O0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 重み（入力層の次元数: 2，隠れ層の次元数: 8，出力層の次元数: 1）\n",
        "W1 = np.random.uniform(low=-0.08, high=0.08, size=(2, 8)).astype(\"float64\")\n",
        "b1 = np.zeros(8).astype(\"float64\")\n",
        "W2 = np.random.uniform(low=-0.08, high=0.08, size=(8, 1)).astype(\"float64\")\n",
        "b2 = np.zeros(1).astype(\"float64\")"
      ],
      "metadata": {
        "id": "HrTd126iCcRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. train関数とvalid関数"
      ],
      "metadata": {
        "id": "ZcG-GIvyDYXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "隠れ層と出力層の2層からなるMLPを実装していきます．\n",
        "\n",
        "**目的関数**\n",
        "\n",
        "負の対数尤度（交差エントロピー）\n",
        "\\begin{equation}\n",
        "E(\\mathbf{x}, \\mathbf{t}) = - \\frac{1}{N} \\sum^N_{i=1}\\left[ \\mathbf{t}_i \\log{\\mathbf{y}_i} + (1 - \\mathbf{t}_i) \\log{(1 - \\mathbf{y_i})}\\right]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "**順伝播**\n",
        "\\begin{align}\n",
        "\\mathbf{u}^{(1)} &= \\mathbf{W}^{(1)T} \\mathbf{x} + \\mathbf{b}^{(1)} \\tag{隠れ層} \\\\\n",
        "\\mathbf{h}^{(1)} &= \\text{ReLU}(\\mathbf{u}^{(1)}) \\tag{隠れ層} \\\\\n",
        "\\mathbf{u}^{(2)} &= \\mathbf{W}^{(2)T} \\mathbf{h}^{(1)} + \\mathbf{b}^{(2)} \\tag{出力層} \\\\\n",
        "\\mathbf{y} &= \\sigma(\\mathbf{u}^{(2)}) \\tag{出力層}\n",
        "\\end{align}\n",
        "\n",
        "**逆伝播**\n",
        "\\begin{align}\n",
        "\\delta^{(2)} &= \\mathbf{y} - \\mathbf{t} \\tag{出力層} \\\\\n",
        "\\delta^{(1)} &= \\text{ReLU}'(\\mathbf{u}^{(1)}) \\odot (\\mathbf{W}^{(2)T} \\delta^{(2)}) \\tag{隠れ層}\n",
        "\\end{align}\n",
        "\n",
        "**勾配の計算**\n",
        "\\begin{align}\n",
        "\\nabla_{\\mathbf{W}^{(1)}}E &= \\frac{1}{N}\\delta^{(1)}\\mathbf{x}^T \\tag{隠れ層} \\\\\n",
        "\\nabla_{\\mathbf{b}^{(1)}}E &= \\frac{1}{N}\\delta^{(1)}\\mathbb{1}_N \\tag{隠れ層} \\\\\n",
        "\\nabla_{\\mathbf{W}^{(2)}}E &= \\frac{1}{N}\\delta^{(2)}\\mathbf{h}^{(1)T} \\tag{出力層} \\\\\n",
        "\\nabla_{\\mathbf{b}^{(2)}}E &= \\frac{1}{N}\\delta^{(2)}\\mathbb{1}_N \\tag{出力層}\n",
        "\\end{align}\n",
        "\n",
        "**重みの更新**\n",
        "\\begin{align}\n",
        "\\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\epsilon \\nabla_{\\mathbf{W}^{(1)}} E \\tag{隠れ層} \\\\\n",
        "\\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\epsilon \\nabla_{\\mathbf{b}^{(1)}} E \\tag{隠れ層} \\\\\n",
        "\\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\epsilon \\nabla_{\\mathbf{W}^{(2)}} E \\tag{出力層} \\\\\n",
        "\\mathbf{b}^{(2)} \\leftarrow \\mathbf{b}^{(2)} - \\epsilon \\nabla_{\\mathbf{b}^{(2)}} E \\tag{出力層}\n",
        "\\end{align}\n",
        "\n",
        "----\n",
        "\n",
        "train_xor ⇒ 学習する関数\n",
        "\n",
        "valid_xor ⇒ テストする関数\n",
        "\n",
        "for loop ⇒ 学習を何回も繰り返す（エポック）"
      ],
      "metadata": {
        "id": "FmpRvsU_DkVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logの中身が0になることを防ぐ\n",
        "def np_log(x):\n",
        "    return np.log(np.clip(x, 1e-10, 1e+10))"
      ],
      "metadata": {
        "id": "JvI5FXhbPOF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xor(x, t, eps):\n",
        "    \"\"\"\n",
        "    :param x: np.ndarray, 入力データ, (batch_size, 入力層の次元数)\n",
        "    :param t: np.ndarray, 教師ラベル, (batch_size, 出力層の次元数)\n",
        "    :param eps: float, 学習率\n",
        "    \"\"\"\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # 順伝播\n",
        "    u1 = np.matmul(x, W1) + b1  # (batch_size, 隠れ層の次元数)\n",
        "    h1 = relu(u1)\n",
        "\n",
        "    u2 = np.matmul(h1, W2) + b2  # (batch_size, 出力層の次元数)\n",
        "    y = sigmoid(u2)\n",
        "\n",
        "    # 誤差の計算\n",
        "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
        "\n",
        "    # 逆伝播\n",
        "    delta_2 = y-t  # (batch_size, 出力層の次元数)\n",
        "    delta_1 = deriv_relu(u1)*np.matmul(delta_2, W2.T)  # (batch_size, 隠れ層の次元数)\n",
        "\n",
        "    # 勾配の計算\n",
        "    dW1 = np.matmul(x.T, delta_1)/batch_size # (入力層の次元数, 隠れ層の次元数)\n",
        "    db1 = np.matmul(np.ones(batch_size), delta_1) / batch_size  # (隠れ層の次元数,)\n",
        "\n",
        "    dW2 = np.matmul(h1.T, delta_2) / batch_size  # (隠れ層の次元数, 出力層の次元数)\n",
        "    db2 = np.matmul(np.ones(batch_size), delta_2) / batch_size  # (出力層の次元数,)\n",
        "\n",
        "     # パラメータの更新\n",
        "    W1 -= eps * dW1\n",
        "    b1 -= eps * db1\n",
        "\n",
        "    W2 -= eps * dW2\n",
        "    b2 -= eps * db2\n",
        "\n",
        "    return cost\n",
        "\n",
        "def valid_xor(x, t):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    # 順伝播\n",
        "    u1 = np.matmul(x, W1) + b1\n",
        "    h1 = relu(u1)\n",
        "\n",
        "    u2 = np.matmul(h1, W2) + b2\n",
        "    y = sigmoid(u2)\n",
        "\n",
        "    # 誤差の計算\n",
        "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
        "\n",
        "    return cost, y"
      ],
      "metadata": {
        "id": "rOYS37oDPVEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. 学習"
      ],
      "metadata": {
        "id": "Ep9LqYJtPl_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3000):\n",
        "     # 訓練データ (x_train_xor, t_train_xor) を1個ずつ取り出して\n",
        "    for x, t in zip(x_train_xor, t_train_xor):\n",
        "      # 1個ずつ train_xor 関数で学習する（パラメータ更新する）\n",
        "      cost = train_xor(x[None, :], t[None, :], eps=0.05)\n",
        "\n",
        "# 全部学習したあとで\n",
        "cost, y_pred = valid_xor(x_valid_xor, t_valid_xor)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeIR5sbZPoh6",
        "outputId": "31acb28f-2af5-4fff-ea3a-79aca42b6286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.99905291]\n",
            " [0.99902943]\n",
            " [0.0082319 ]\n",
            " [0.00103143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.【課題 2】多層パーセプトロンの実装と学習(MNIST)"
      ],
      "metadata": {
        "id": "fdEpBD--P8fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. ソフトマックス関数\n",
        "ソフトマックス関数は多クラス分類の出力層で用いられる活性化関数です．こちらも勾配の計算に利用するために導関数も実装していきます．  \n",
        "\n",
        "\\begin{equation}\n",
        "\\text{softmax}(\\mathbf{x})_k = \\frac{\\text{exp}(\\mathbf{x}_k)}{\\sum^K_{k'=1} \\text{exp}(\\mathbf{x}_{k'})} \\quad \\quad \\text{for} \\quad k=1, \\dots K\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{softmax}'(\\mathbf{x})_k = \\text{softmax}(\\mathbf{x})_k (1 - \\text{softmax}(\\mathbf{x}))_k\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "UDUGHs8TfXH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    x -= x.max(axis=1, keepdims=True)  # オーバーフローを避ける\n",
        "    x_exp = np.exp(x)\n",
        "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def deriv_softmax(x):\n",
        "    return softmax(x) * (1 - softmax(x))"
      ],
      "metadata": {
        "id": "9cubfAAegHIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. データセットの設定\n",
        "次にデータセットを作成します．ここでは第2回の演習でも利用したMNISTデータセットを用います．データセット・データの前処理に関する説明については第2回の演習をご参考ください．"
      ],
      "metadata": {
        "id": "vTArTuMYgYDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_mnist_1, t_mnist_1), (x_mnist_2, t_mnist_2) = mnist.load_data()\n",
        "\n",
        "x_mnist = np.r_[x_mnist_1, x_mnist_2]\n",
        "t_mnist = np.r_[t_mnist_1, t_mnist_2]\n",
        "\n",
        "x_mnist = x_mnist.astype(\"float64\") / 255.  # 値を[0, 1]に正規化する\n",
        "t_mnist = np.eye(N=10)[t_mnist.astype(\"int32\").flatten()]  # one-hotベクトルにする\n",
        "\n",
        "x_mnist = x_mnist.reshape(x_mnist.shape[0], -1)  # 1次元に変換\n",
        "\n",
        "# train data: 5000, valid data: 10000, test data: 10000にする\n",
        "x_train_mnist, x_test_mnist, t_train_mnist, t_test_mnist =\\\n",
        "    train_test_split(x_mnist, t_mnist, test_size=10000)\n",
        "x_train_mnist, x_valid_mnist, t_train_mnist, t_valid_mnist =\\\n",
        "    train_test_split(x_train_mnist, t_train_mnist, test_size=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLJBpp9EgdIP",
        "outputId": "354bfc9c-1e66-48e9-8698-ed44b01aa897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. 全結合層の定義  \n",
        "\n",
        "多層のMLPを実装できるように，全結合層をクラスとして定義します．\n",
        "\n",
        "順伝播，逆伝播，勾配の計算をそれぞれ関数として実装します．\n",
        "\n",
        "数式は以下のようになります．  \n",
        "\n",
        "**順伝播**(`__call__`)\n",
        "\\begin{align}\n",
        "\\mathbf{u}^{(j)} &= \\mathbf{W}^{(j)T}\\mathbf{h}^{(j-1)} + \\mathbf{b}^{(j)}  \\\\\n",
        "\\mathbf{h}^{(j)} &= \\text{function}(\\mathbf{u}^{(j)})\n",
        "\\end{align}\n",
        "\n",
        "**逆伝播**(`b_prop`)\n",
        "\\begin{equation}\n",
        "\\delta^{(j)} = \\text{function}'(\\mathbf{u}^{(j)}) \\odot (\\mathbf{W}^{(j+1)T} \\delta^{(j+1)})\n",
        "\\end{equation}\n",
        "\n",
        "**勾配の計算**(`compute_grad`)\n",
        "\\begin{align}\n",
        "\\nabla_{\\mathbf{W}^{(j)}E} &= \\frac{1}{N}\\delta^{(j)}\\mathbf{h}^{(j)T} \\\\\n",
        "\\nabla_{\\mathbf{b}^{(j)}E} &= \\frac{1}{N}\\delta^{(j)}\\mathbb{1}_N\n",
        "\\end{align}\n",
        "\n",
        "`__call__`は，インスタンスを関数のように呼び出すための特殊メソッドです．\n",
        "```python\n",
        "Class A:\n",
        "  def __init__(self):\n",
        "    ...\n",
        "  def __call__(self):\n",
        "    print('Hello World.')\n",
        "\n",
        "a = A()\n",
        "a()  # __call__が呼び出される\n",
        "# Hello world.\n",
        "```\n",
        "\n",
        "`get_params`，`set_params`，`get_grads`はそれぞれ重み，勾配をベクトルで受け渡す関数です．課題3の勾配チェックの際に使用します．  "
      ],
      "metadata": {
        "id": "UQ75UXddhar_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**そもそもselfってなに？？**\n",
        "\n",
        "たとえば、こういうクラスがあったとするよ：\n",
        "\n",
        "    class Cat:\n",
        "         def __init__(self, name):\n",
        "            self.name = name\n",
        "この時、\n",
        "\n",
        "    a = Cat(\"Tama\")\n",
        "    b = Cat(\"Momo\")\n",
        "\n",
        "ってすると、\n",
        "\n",
        "a.name は \"Tama\"、b.name は \"Momo\"\n",
        "\n",
        "同じように、\n",
        "\n",
        "Dense というクラスから作ったインスタンス（例えば1層目、2層目）が、\n",
        "\n",
        "それぞれ 自分専用の重み (W)\n",
        "\n",
        "それぞれ 自分専用のバイアス (b)\n",
        "\n",
        "それぞれ 自分専用の計算履歴 (self.u, self.x)\n",
        "\n",
        "を持つ必要があるよね。\n",
        "\n",
        "だから、クラスの中で「このインスタンス自身が持つもの」には必ず self. を付けるっていうルールになっているんだ！\n",
        "\n",
        "    first = Dense(入力次元, 隠れ層次元, 活性化関数, その微分)\n",
        "    second = Dense(隠れ層次元, 出力層次元, 活性化関数, その微分)\n",
        "\n",
        "みたいにしておけば、\n",
        "\n",
        "    first.W は「1層目の重み」\n",
        "\n",
        "    first.b は「1層目のバイアス」\n",
        "\n",
        "    second.W は「2層目の重み」\n",
        "\n",
        "    second.b は「2層目のバイアス」\n",
        "\n",
        "みたいに簡単に変数を取得できる！\n"
      ],
      "metadata": {
        "id": "_F-uxe12Htje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この Dense クラスは、\n",
        "**ニューラルネットワークの「1層」**を表している"
      ],
      "metadata": {
        "id": "wddft9GDF08x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "    __init__\n",
        "\n",
        "（インスタンスが作られたときに呼ばれる）\n",
        "\n",
        "**Cで言うコンストラクタ！！**\n",
        "\n",
        "W: 重み行列をランダムに初期化（小さい値で）\n",
        "\n",
        "b: バイアスをゼロで初期化\n",
        "\n",
        "function: 活性化関数（例：シグモイド、ReLU）\n",
        "\n",
        "deriv_function: 活性化関数の微分（学習時に必要）\n",
        "\n",
        "ここで「この層の形」と「使う関数」が決まる！\n",
        "\n",
        "---\n",
        "    __call__(self, x)\n",
        "\n",
        "（インスタンスを呼び出すとき＝順伝播の計算）\n",
        "\n",
        "入力 x が来たら、u = xW + b を計算して（線形変換）活性化関数を通して h = f(u) にする\n",
        "\n",
        "それを次の層に渡す（もしくは最終出力にする）\n",
        "\n",
        "---\n",
        "\n",
        "    b_prop(self, delta, W)\n",
        "（誤差を逆向きに伝える＝逆伝播）\n",
        "\n",
        "次の層から来た誤差 delta を受け取って、\n",
        "\n",
        "この層でどれくらい間違ったか（勾配）を計算する。\n",
        "\n",
        "ここで「活性化関数の微分」を使ってるよ。\n",
        "\n",
        "---\n",
        "    compute_grad(self)\n",
        "（勾配の具体的な数値を計算する）\n",
        "\n",
        "誤差を使って、重みWの勾配 dW and バイアスbの勾配 db を実際に数式で計算する。\n",
        "\n",
        "---\n",
        "    get_params(self)\n",
        "\n",
        "今の重みWとバイアスbを、1列にまとめて取り出す。\n",
        "\n",
        "---\n",
        "\n",
        "    set_params(self, params)\n",
        "\n",
        "まとめて受け取ったパラメータから、重みWとバイアスbに分けてセットする。\n",
        "\n",
        "---\n",
        "\n",
        "    get_grads(self)\n",
        "\n",
        "計算した勾配（dWとdb）を1列にまとめて取り出す。"
      ],
      "metadata": {
        "id": "1nNXQ3DwGNpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
        "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
        "                                   size=(in_dim, out_dim)).astype(\"float64\")\n",
        "        self.b = np.zeros(out_dim).astype(\"float64\")\n",
        "        self.function = function\n",
        "        self.deriv_function = deriv_function\n",
        "\n",
        "        self.x = None\n",
        "        self.u = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        self.params_idxs = np.cumsum([self.W.size, self.b.size])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        順伝播処理を行うメソッド．\n",
        "        x: (batch_size, in_dim_{j})\n",
        "        h: (batch_size, out_dim_{j})\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.u = np.matmul(self.x, self.W) + self.b\n",
        "        h = self.function(self.u)\n",
        "        return h\n",
        "\n",
        "    def b_prop(self, delta, W):\n",
        "        \"\"\"\n",
        "        誤差逆伝播を行うメソッド．\n",
        "        delta (=delta_{j+1}): (batch_size, out_dim_{j+1})\n",
        "        W (=W_{j+1}): (out_dim_{j}, out_dim_{j+1})\n",
        "        self.delta (=delta_{j}): (batch_size, out_dim_{j})\n",
        "        \"\"\"\n",
        "        self.delta = self.deriv_function(self.u) * np.matmul(delta, W.T)\n",
        "        return self.delta\n",
        "\n",
        "    def compute_grad(self):\n",
        "        \"\"\"\n",
        "        勾配を計算するメソッド．\n",
        "        self.x: (batch_size, in_dim_{j})\n",
        "        self.delta: (batch_size, out_dim_{j})\n",
        "        self.dW: (in_dim_{j}, out_dim_{j})\n",
        "        self.db: (out_dim_{j})\n",
        "        \"\"\"\n",
        "        batch_size = self.delta.shape[0]\n",
        "\n",
        "        self.dW = np.matmul(self.x.T, self.delta) / batch_size  # (隠れ層の次元数, 出力層の次元数)\n",
        "        self.db = np.matmul(np.ones(batch_size), self.delta) / batch_size  # (出力層の次元数,)\n",
        "\n",
        "    def get_params(self):\n",
        "        return np.concatenate([self.W.ravel(), self.b], axis=0)\n",
        "\n",
        "    def set_params(self, params):\n",
        "        \"\"\"\n",
        "        params: List[np.ndarray, np.ndarray]\n",
        "            1つ目の要素が重みW: (in_dim, out_dim)，2つ目の要素がバイアス: (out_dim,)\n",
        "        \"\"\"\n",
        "        _W, _b = np.split(params, self.params_idxs)[:-1]\n",
        "        self.W = _W.reshape(self.W.shape)\n",
        "        self.b = _b\n",
        "\n",
        "    def get_grads(self):\n",
        "        return np.concatenate([self.dW.ravel(), self.db], axis=0)"
      ],
      "metadata": {
        "id": "yuAPucgki9wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このDenseクラスを用いてモデル全体をModelクラスとして実装します．今後利用する深層学習ライブラリのPyTorchでも似たようなモデルの定義をするので今のうちに慣れておきましょう．  \n",
        "\n",
        "---\n",
        "\n",
        "Denseは一層だけだったけど、Modelでは層を重ねて全体のニューラルネットワークを構築する！\n",
        "\n",
        "*Modelの中で、Denseの情報ってどこから来てるの？どうやってDenseの関数や変数を使ってるの？*\n",
        "\n",
        "➡ Modelクラスは、自分でDenseインスタンスを作って、それを使っている！\n",
        "\n",
        "つまり\n",
        "    \n",
        "    self.layers.append(Dense(入力, 出力, 活性化関数, 微分関数))\n",
        "\n",
        "この行で、自分でDenseのオブジェクト（＝実体）を作って、ループを使って、そのオブジェクト（＝1個1個の層）を、self.layersというリストに全部保存してるんだ！\n",
        "\n",
        "だから、あとはこのリストを使って、\n",
        "Denseが持ってる関数や変数を自由に呼び出せる！！"
      ],
      "metadata": {
        "id": "8kpeZlEGCaTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, hidden_dims, activation_functions, deriv_functions):\n",
        "        \"\"\"\n",
        "        :param hiden_dims: List[int]，各層のノード数を格納したリスト．\n",
        "        :params activation_functions: List, 各層で用いる活性化関数を格納したリスト．\n",
        "        :params derive_functions: List，各層で用いる活性化関数の導関数を格納したリスト．\n",
        "        \"\"\"\n",
        "        # 各層をリストに格納していく\n",
        "        self.layers = []\n",
        "        for i in range(len(hidden_dims)-2):  # 出力層以外は同じ構造\n",
        "            self.layers.append(Dense(hidden_dims[i], hidden_dims[i+1],\n",
        "                                     activation_functions[i], deriv_functions[i]))\n",
        "        self.layers.append(Dense(hidden_dims[-2], hidden_dims[-1],\n",
        "                                 activation_functions[-1], deriv_functions[-1]))  # 出力層を追加\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"順伝播処理を行うメソッド\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, delta):\n",
        "        \"\"\"誤差逆伝播，勾配計算を行うメソッド\"\"\"\n",
        "        batch_size = delta.shape[0]\n",
        "\n",
        "        for i, layer in enumerate(self.layers[::-1]):\n",
        "            if i == 0:  # 出力層の場合\n",
        "                layer.delta = delta  # y - t\n",
        "                layer.compute_grad()\n",
        "            else:  # 出力層以外の場合\n",
        "                delta = layer.b_prop(delta, W)  # 逆伝播\n",
        "                layer.compute_grad()  # 勾配の計算\n",
        "\n",
        "            W = layer.W\n",
        "\n",
        "    def update(self, eps=0.01):\n",
        "        \"\"\"パラメータの更新を行うメソッド\"\"\"\n",
        "        for layer in self.layers:\n",
        "            layer.W -= eps * layer.dW\n",
        "            layer.b -= eps * layer.db"
      ],
      "metadata": {
        "id": "aqt5BqilCo1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qNokS1SBOEt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にモデルを利用するときはこのクラスのインスタンスを作成します．本課題では3層のMLPを用います．  \n",
        "\n",
        "hidden_dimsで以下のような層の流れを作る\n",
        "\n",
        "    (784次元) → (100次元) → (100次元) → (10次元)\n",
        "\n",
        "それから、activation_functionsで、以下の関数を使うことを指示\n",
        "\n",
        "    1層目: ReLU　2層目: ReLU　出力層: Softmax\n",
        "\n",
        "deriv_functionsでそれぞれの層に対応する微分式を指示。\n"
      ],
      "metadata": {
        "id": "Izjg7ZSEEyoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(hidden_dims=[784, 100, 100, 10],\n",
        "              activation_functions=[relu, relu, softmax],\n",
        "              deriv_functions=[deriv_relu, deriv_relu, deriv_softmax])"
      ],
      "metadata": {
        "id": "m0VnNqrzE7Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "課題1ではデータを1つずつ渡すオンライン学習を用いましたが，ここではミニバッチ学習を用います．そのためにデータセットをミニバッチに分割する関数を定義します．  "
      ],
      "metadata": {
        "id": "-FjIR6qZIL1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(data, batch_size):\n",
        "    \"\"\"\n",
        "    :param data: np.ndarray，入力データ\n",
        "    :param batch_size: int，バッチサイズ\n",
        "    \"\"\"\n",
        "    num_batches, mod = divmod(data.shape[0], batch_size)\n",
        "    batched_data = np.split(data[: batch_size * num_batches], num_batches)\n",
        "    if mod:\n",
        "        batched_data.append(data[batch_size * num_batches:])\n",
        "\n",
        "    return batched_data"
      ],
      "metadata": {
        "id": "pwgtWxdCIMpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. train関数とvalid関数\n",
        "\n",
        "**誤差関数**  \n",
        "\n",
        "負の対数尤度（多クラス交差エントロピー）\n",
        "\\begin{equation}\n",
        "E(\\mathbf{x}, \\mathbf{t}) = - \\frac{1}{N} \\sum^N_{i=1} \\sum^K_{k=1}\\mathbf{t}_{i, k} \\log{\\mathbf{y}_{i, k}}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "mK7lR2Q-lc5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の損失関数と，先に定義したModelクラスのインスタンスを用いて，モデルを訓練するための関数を定義します．この関数では1エポック分の訓練を行います．"
      ],
      "metadata": {
        "id": "tWNxa_1SJoBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mst(model, x, t, eps=0.01):\n",
        "    # 順伝播\n",
        "    y = model(x)\n",
        "\n",
        "    # 誤差の計算\n",
        "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
        "\n",
        "    # 逆伝播\n",
        "    delta = y - t\n",
        "    model.backward(delta)\n",
        "\n",
        "    # パラメータの更新\n",
        "    model.update(eps)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "Oi_r6wsBl2gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "同様に訓練したモデルを評価するための関数を定義します．関数の中身は訓練とほとんど変わりませんが，評価時には誤差逆伝播による勾配の計算と，パラメータの更新が不要になります．  "
      ],
      "metadata": {
        "id": "_-5n5Jg5JyNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_mst(model, x, t):\n",
        "    # 順伝播\n",
        "    y = model(x)\n",
        "\n",
        "    # 誤差の計算\n",
        "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
        "\n",
        "    return cost, y"
      ],
      "metadata": {
        "id": "JfiV2dfPmHPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. 学習  "
      ],
      "metadata": {
        "id": "n_O-NCslmW3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# バッチサイズを指定\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(10):\n",
        "    x_train_mnist, t_train_mnist = shuffle(x_train_mnist, t_train_mnist)\n",
        "    x_train_batch, t_train_batch = \\\n",
        "        create_batch(x_train_mnist, batch_size), create_batch(t_train_mnist, batch_size)\n",
        "\n",
        "    # ミニバッチ学習\n",
        "    for x, t in zip(x_train_batch, t_train_batch):\n",
        "        cost = train_mst(model, x, t, eps=0.1)\n",
        "\n",
        "    cost, y_pred = valid_mst(model, x_valid_mnist, t_valid_mnist)\n",
        "    accuracy = accuracy_score(t_valid_mnist.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "    print(f\"EPOCH: {epoch+1} Valid Cost: {cost:.3f} Valid Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZdTe6oJmYT1",
        "outputId": "14300edf-6a4f-4fc2-bee4-1ba33f57e265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1 Valid Cost: 0.353 Valid Accuracy: 0.896\n",
            "EPOCH: 2 Valid Cost: 0.263 Valid Accuracy: 0.922\n",
            "EPOCH: 3 Valid Cost: 0.205 Valid Accuracy: 0.939\n",
            "EPOCH: 4 Valid Cost: 0.186 Valid Accuracy: 0.943\n",
            "EPOCH: 5 Valid Cost: 0.152 Valid Accuracy: 0.955\n",
            "EPOCH: 6 Valid Cost: 0.129 Valid Accuracy: 0.962\n",
            "EPOCH: 7 Valid Cost: 0.152 Valid Accuracy: 0.954\n",
            "EPOCH: 8 Valid Cost: 0.118 Valid Accuracy: 0.964\n",
            "EPOCH: 9 Valid Cost: 0.107 Valid Accuracy: 0.968\n",
            "EPOCH: 10 Valid Cost: 0.105 Valid Accuracy: 0.968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.Tips:実験の可視化\n",
        "\n",
        "通常，lossやaccuracyなどのログは，数値だけで追うのはわかりにくいため，グラフで可視化して確認します．\n",
        "\n",
        "その際によく使われるツールとして，[Weights & Biases](https://wandb.ai/site/ja/)と[Tensorboard](https://www.tensorflow.org/tensorboard?hl=ja)が挙げられます．\n",
        "\n",
        "\n",
        "- Weights & Biases: クラウドベースの実験管理ツールで，非常に多機能です．可視化だけでなく，複数の実験の比較やハイパーパラメータの自動チューニング，モデルのチェックポイント保存なども可能です．利用するにはアカウント登録が必要です．\n",
        "- Tensorboard: TensorFlow公式の可視化ツールで，ローカル環境で簡単に使うことができます．\n",
        "\n",
        "演習では扱いませんが，宿題や最終課題に取り組む際に積極的に活用してみてください．\n",
        "\n",
        "Google Colabでの使い方は下記を参照してください．\n",
        "\n",
        "- [Weights & Biasesのチュートリアル](https://docs.wandb.ai/ja/tutorials)\n",
        "- [Tensorboardのチュートリアル（TensorFlow）](https://www.tensorflow.org/tensorboard/get_started?hl=ja)\n",
        "- [TensorboardをPytorchで使う方法](https://qiita.com/go50/items/ae5f979b9bb36bfbb6be)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1fkAlZBNJpyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.【課題 3】数値微分（勾配チェック）  \n",
        "\n",
        "誤差逆伝播法による勾配の計算は少し複雑なため，実装にバグが入りがちです．\n",
        "\n",
        "実装が簡単な数値微分と結果を比較することで，逆伝播の実装が正しいかを確認してみましょう．  "
      ],
      "metadata": {
        "id": "WA98nAv1mxWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. 1変数の場合\n",
        "\n",
        "まず簡単な2次関数に対して数値微分を行ってみましょう．  "
      ],
      "metadata": {
        "id": "cTFnh6oxofw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return x ** 2\n",
        "\n",
        "\n",
        "def deriv_f(x):\n",
        "    return 2 * x"
      ],
      "metadata": {
        "id": "jZm8yb1UomkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1変数の場合の数値微分の式は以下のようになります．  \n",
        "\n",
        "\\begin{equation}\n",
        "f'(x) = \\underset{h \\rightarrow 0}{\\text{lim}} \\frac{f(x + h) - f(x - h)}{2h}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "BSbMYmfyoqKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 1e-5\n",
        "x = 2.0\n",
        "\n",
        "grad_auto = deriv_f(x)\n",
        "grad_num = (f(x + eps) - f(x - eps)) / (2 * eps)\n",
        "\n",
        "print(grad_auto, grad_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv9JGSOKpAXA",
        "outputId": "2ff5b4ca-99f1-4815-9354-c060714d04f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0 4.000000000026205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. 多変数の場合(MLP)\n",
        "次に課題2で定義したMLPに対して数値微分の計算を行い，誤差逆伝播による勾配(`dW`, `db`)の計算が間違っていないかを確認してみましょう．\n",
        "\n",
        "多変数(MLP)の場合の数値微分の式は次のようになります．ある1つの変数$\\theta_m$のみを$h$だけ動かした場合を考えます．  \n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E}{\\partial \\theta_m} = \\underset{h \\rightarrow 0}{\\text{lim}}\\frac{E(\\theta_1, \\theta_2, \\dots , \\theta_m + h, \\dots , \\theta_M) - E(\\theta_1, \\theta_2, \\dots , \\theta_m - h, \\dots , \\theta_M)}{2h}\n",
        "\\end{equation}\n",
        "\n",
        "実装では，変数全体のサイズのゼロベクトルを用意し，$m$番目の要素のみ$h$だけずらされたベクトルを作り，それに対応する誤差を計算し，そこから上の式に従って最終的な微分の値を求めていきます．  \n",
        "\n",
        "まず各層ごとの重みをベクトルで取得しリストで保存していく関数を実装していきます．  \n",
        "\n",
        "MLPの各レイヤーから重みをベクトルで取得するために，先ほど`Dense`クラスで定義した`set_params`，`get_params`を使用します．  "
      ],
      "metadata": {
        "id": "wCqJgtmipLrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(layers):\n",
        "    params_all = []\n",
        "    for layer in layers:\n",
        "        params = layer.get_params()\n",
        "        params_all.append(params)\n",
        "\n",
        "    return params_all"
      ],
      "metadata": {
        "id": "U0lpoIlsqJfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_params(laeyrs, params_all):\n",
        "    for layer, params in zip(model.layers, params_all):\n",
        "        layer.set_params(params)"
      ],
      "metadata": {
        "id": "nehSAkCtqRBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, t):\n",
        "    # 順伝播\n",
        "    y = model(x)\n",
        "\n",
        "    # 誤差の計算\n",
        "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "LT1fV2aSqVx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1. 数値微分\n",
        "\n",
        "勾配の計算に使用するデータを用意します．勾配チェックの際はどれだけ精度よく近似できているかを見たいので，`float64`を使いましょう．  "
      ],
      "metadata": {
        "id": "jtc0XoQaqhyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "x = x_train_mnist[:batch_size].astype(\"float64\")\n",
        "t = t_train_mnist[:batch_size].astype(\"float64\")"
      ],
      "metadata": {
        "id": "GVYwUiUvqwww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 1e-5\n",
        "\n",
        "params_all = get_params(model.layers)\n",
        "grads_all_num = []\n",
        "\n",
        "# レイヤーごとに勾配を計算\n",
        "for layer, params in zip(model.layers, params_all):\n",
        "    shift = np.zeros_like(params)\n",
        "    grads_num = np.zeros_like(params)\n",
        "\n",
        "    # レイヤー内のM個のパラメータに対してそれぞれ微分を計算する\n",
        "    for m in range(len(params)):\n",
        "        shift[m] = eps  # m番目のパラメータのみeps分ずらす [0, 0, ..., 0, eps, 0, ..., 0]\n",
        "\n",
        "        params_right = params + shift\n",
        "        layer.set_params(params_right)\n",
        "        cost_right = compute_cost(x, t)  # L(x; ..., \\theta_m + eps, ...)\n",
        "\n",
        "        params_left = params - shift\n",
        "        layer.set_params(params_left)\n",
        "        cost_left = compute_cost(x, t)  # L(x; ..., \\theta_m - eps, ...)\n",
        "\n",
        "        grads_num[m] = (cost_right - cost_left) / (2 * eps)  # 微分の計算\n",
        "\n",
        "        layer.set_params(params)\n",
        "        shift[m] = 0\n",
        "\n",
        "    grads_all_num.append(grads_num)"
      ],
      "metadata": {
        "id": "D4TvumnLq20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2. 誤差逆伝播法"
      ],
      "metadata": {
        "id": "njAOv63or0pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grads(layers):\n",
        "    grads_all = []\n",
        "    for layer in layers:\n",
        "        grads = layer.get_grads()\n",
        "        grads_all.append(grads)\n",
        "\n",
        "    return grads_all"
      ],
      "metadata": {
        "id": "EHEBk04asHeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 順伝播\n",
        "y = model(x)\n",
        "\n",
        "# 逆伝播\n",
        "delta = y - t\n",
        "model.backward(delta)\n",
        "\n",
        "# 勾配を計算\n",
        "grads_all_bprop = get_grads(model.layers)"
      ],
      "metadata": {
        "id": "sSiADsqesORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.3. 比較（勾配チェック）\n",
        "\n",
        "誤差逆伝播法で計算した勾配と数値微分による勾配の差を，ノルムで正規化したrelative differenceで測ります．経験的にはその差がおおよそ1e-7以下であれば実装にバグはないと安心していいでしょう [1]．\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{diff} = \\frac{||\\text{grad}_{\\text{bprop}} - \\text{grad}_{\\text{num}}||_2}{||\\text{grad}_{\\text{bprop}}||_2 + ||\\text{grad}_{\\text{num}}||_2}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "[1] Improving Deep Neural Networks: Gradient checking: https://www.coursera.org/lecture/deep-neural-network/gradient-checking-htA0l (2018年10月17日閲覧)"
      ],
      "metadata": {
        "id": "EF2AUFdtsW4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (grads_bprop, grads_num) in enumerate(zip(grads_all_bprop, grads_all_num)):\n",
        "    diff = np.linalg.norm(grads_bprop - grads_num) / (np.linalg.norm(grads_bprop) + np.linalg.norm(grads_num))\n",
        "    print(f\"Gradients' difference (layer {i+1}: {diff})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5cbNK-xtKxx",
        "outputId": "ceadfd5f-5c0c-4958-d3b2-b10b7c84d593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients' difference (layer 1: 1.6283271451528477e-10)\n",
            "Gradients' difference (layer 2: 1.3802083643379515e-10)\n",
            "Gradients' difference (layer 3: 6.181276638842828e-11)\n"
          ]
        }
      ]
    }
  ]
}