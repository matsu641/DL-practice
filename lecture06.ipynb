{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matsu641/DL-practice/blob/main/lecture06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXAHZIBIRmL0"
      },
      "source": [
        "# 第6回講義 演習\n",
        "\n",
        "今回は，FCN（Fully Convolutional Network）を実装し，VOC2011データセットにSegmentationを実行します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 目次\n",
        "[【課題】FCNによる segmentation モデルの実装と学習](#scrollTo=_pBFN4mD934j)\n",
        "1. [VOC2011 データセットの読み込みと可視化](#scrollTo=d304y5XwRvGG)\n",
        "2. [Preprocessing](#scrollTo=Lamm5y2CT2-W)\n",
        "3. [FCN の実装](#scrollTo=R2yHHDp0VUJR)\n",
        "\n",
        "  3.1 [backbone （事前学習モデル） の利用](#scrollTo=k4regropVqgq)\n",
        "\n",
        "  3.2 [FCN クラスの実装](#scrollTo=KHaV8k4mXrHm)\n",
        "\n",
        "4. [FCN の学習](#scrollTo=igJrjMAjasf_)\n"
      ],
      "metadata": {
        "id": "idnwFkGW-DOp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M5gFY4mUbK8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 【課題】FCNによる segmentation モデルの実装と学習"
      ],
      "metadata": {
        "id": "_pBFN4mD934j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d304y5XwRvGG"
      },
      "source": [
        "## 1.VOC2011 データセットの読み込みと可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlA62sBOR4KL"
      },
      "source": [
        "今回の演習で扱う**VOC2011**は，以下の3用途に用いることができる，画像認識タスク用のデータセットとなっています．\n",
        "- Classification\n",
        "- Segmentation\n",
        "- Object detection\n",
        "\n",
        "Segmentationタスク用には，以下が提供されています．\n",
        "- 入力画像\n",
        "- Segmentation mask（ピクセル単位/21クラス）\n",
        "\n",
        "VOC2011 を 前回の CIFAR-10 と同様，PyTorch でダウンロードし描画します．\n",
        "\n",
        "以下では入力画像が左，Segmentation maskが右です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoZhrNhzCDaL"
      },
      "source": [
        "#　VOC2011の可視化\n",
        "train_dataset = VOCSegmentation(root=\"./VOCSegmentation/2011\",\n",
        "                                   year=\"2011\", image_set=\"train\", download=True)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(4):\n",
        "    image, target = train_dataset[i]\n",
        "    plt.subplot(4, 2, 2*i+1)\n",
        "    plt.imshow(image);\n",
        "    plt.subplot(4, 2, 2*i+2)\n",
        "    plt.imshow(target);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lamm5y2CT2-W"
      },
      "source": [
        "## 2.Preprocessing\n",
        "VOCデータセットに以下の前処理を施します\n",
        "- 入力画像，segmentation mask ともに(224, 224)へのリサイズ\n",
        "- segmentation mask の境界線に対応するクラス(255)の除去\n",
        "- クラスラベルのone-hot encoding\n",
        "\n",
        "前処理により，それぞれ以下のような形状になります．\n",
        "- 入力画像: (3, 224, 224)\n",
        "- segmentation mask: (21, 224, 224)\n",
        "\n",
        "本演習では PyTorch に用意されている前処理の中でも画像サイズの変更 (Resize) ，PyTorch の Tensor クラスへの変更と正規化 (ToTensor) のみを利用していますが， PyTorch では様々な前処理が用意されています．以下の公式リファレンスを参考にしてください．\n",
        "https://pytorch.org/vision/main/transforms.html#v1-api-reference  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuut_EbiU2e"
      },
      "source": [
        "batch_size = 16\n",
        "num_classes = 21\n",
        "\n",
        "# 前処理の定義\n",
        "def TargetToTensor(target):\n",
        "    target = np.array(target)\n",
        "    target[target > 20] = 0 # labelを0-20の合計21クラスに限定（objectのエッジを消す）\n",
        "    target = torch.from_numpy(target).type(torch.long)\n",
        "    target = F.one_hot(target, num_classes=num_classes).permute(2,0,1).type(torch.float)\n",
        "    return target  # (21, 224, 224)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # (224, 224) にリサイズ\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda target: TargetToTensor(target))\n",
        "])\n",
        "\n",
        "# datasetの定義\n",
        "train_dataset = VOCSegmentation(root=\"./VOCSegmentation/2011\", year=\"2011\",\n",
        "                                image_set=\"train\", download=True,\n",
        "                                transform=image_transform,\n",
        "                                target_transform=target_transform)\n",
        "valid_dataset = VOCSegmentation(root=\"./VOCSegmentation/2011\", year=\"2011\",\n",
        "                                image_set=\"val\", download=True,\n",
        "                                transform=image_transform,\n",
        "                                target_transform=target_transform)\n",
        "\n",
        "# dataloaderの定義\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2yHHDp0VUJR"
      },
      "source": [
        "## 3.FCNの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MMgxrFKViu1"
      },
      "source": [
        "FCNは以下の二段階に分かれます．\n",
        "- 既存の画像認識モデル（backbone）による特徴抽出\n",
        "- ConvolutionとUpscaleによるピクセル単位の予測"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 backbone（事前学習モデル）の利用\n",
        "\n",
        "今回はbackboneとして以下の事前学習モデルを採用します．\n",
        "- ResNet18\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture.png\" alt=\"ResNet18\" width=600 >\n",
        "\n",
        "https://www.researchgate.net/publication/336642248_A_Deep_Learning_Approach_for_Automated_Diagnosis_and_Multi-Class_Classification_of_Alzheimer's_Disease_Stages_Using_Resting-State_fMRI_and_Residual_Neural_Networks\n",
        "\n",
        "- GAP層（Global Average Pooling層）とFC層を外し，特徴抽出器とする\n",
        "\n",
        "\n",
        " backboneの入出力(CHW)は以下のようになります．\n",
        " - 入力: (3, 224, 224)\n",
        " - 出力: (512, 7, 7)\n",
        "\n",
        "\n",
        " PyTorch では様々な事前学習モデルが実装されています．タスクの複雑さやダウンストリームとの相性，利用可能な計算資源に応じて使い分けることが可能です．\n",
        "\n",
        " 参考: https://pytorch.org/vision/main/models.html"
      ],
      "metadata": {
        "id": "k4regropVqgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backbone の作成\n",
        "resnet18 = torchvision.models.resnet18(pretrained=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "resnet18 = nn.Sequential(*list(resnet18.children())[:-2])  # GAP層とFC層を外す\n",
        "\n",
        "# backbone の出力サイズを確認\n",
        "dummy = torch.rand(1, 3, 224, 224)  # (B, C, H, W)\n",
        "resnet18(dummy).shape"
      ],
      "metadata": {
        "id": "CaI2Ve23vsNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHaV8k4mXrHm"
      },
      "source": [
        "### 3.2 FCNクラスの実装\n",
        "FCNでは，以下の手順で予測を出力します．\n",
        "- Convolution: backboneで得られた特徴をクラス数のchannelに変換\n",
        "- Upscale: スケールが元画像に一致するよう変換して，予測とする\n",
        "\n",
        "今回はそれぞれを以下のように実装します．\n",
        "- Convolution: FCNhead\n",
        "  - Conv2d(input_channel, input_channel//4, 3, padding=1, bias=False)\n",
        "  - BatchNorm2d\n",
        "  - ReLU\n",
        "  - Dropout(0.1)\n",
        "  - Conv2d(input_channel//4, num_classes, 1)\n",
        "- Upscale: `F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13uEnnplHmic"
      },
      "source": [
        "# FCNの定義\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=21):\n",
        "        super(FCN, self).__init__()\n",
        "        # backbone\n",
        "        self.backbone = backbone\n",
        "        # convolution\n",
        "        self.FCNhead = # WRITE ME\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:] # shape: (224, 224)\n",
        "        x = # WRITE ME  # backboneでの処理 (512, 7, 7)\n",
        "        x = # WRITE ME  # FCNheadでの処理 (21, 7 ,7 )\n",
        "        x = # WRITE ME  # upscale (21, 224, 224)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igJrjMAjasf_"
      },
      "source": [
        "## 4.FCNの学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7iI_S32bHeX"
      },
      "source": [
        "以上に実装したFCNを，VOCデータセットに対して学習させます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhHzuNu_Jwoa"
      },
      "source": [
        "# modelの作成\n",
        "model = FCN(backbone=resnet18, num_classes=num_classes)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ6aFeVjbjfW"
      },
      "source": [
        "以下のように設定します．\n",
        "- オプティマイザ: AdamW\n",
        "- 損失関数: BinaryCrossEntropy\n",
        "- 評価指標: mean-IoU\n",
        "\n",
        "<img src=\"https://learnopencv.com/wp-content/uploads/2022/08/05-document-segmentation-IoU-Dice-concept.png\" alt=\"miou\" width=600 >\n",
        "\n",
        "https://learnopencv.com/deep-learning-based-document-segmentation-using-semantic-segmentation-deeplabv3-on-custom-dataset/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCd79R3yVNP4"
      },
      "source": [
        "# 下記リンク先のmIoU実装を利用\n",
        "# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n",
        "class mIoUScore(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n",
        "        ).reshape(n_class, n_class)    # ij 成分は，target がクラス i ， 予測がクラス j だったピクセルの数\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n",
        "\n",
        "    def get_scores(self):\n",
        "        hist = self.confusion_matrix\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "        mean_iou = np.nanmean(iou)\n",
        "        return mean_iou\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gniqMNM7ao3M"
      },
      "source": [
        "# optimizer, loss function, metricsの定義\n",
        "loss_fn = # WRITE ME\n",
        "metrics = mIoUScore(num_classes)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPINwLDcCfG"
      },
      "source": [
        " FCNの学習を実行します．  \n",
        " google colab の無料枠で数分で終わります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1NGbmJ-Dh-C"
      },
      "source": [
        "n_epochs = 5\n",
        "\n",
        "# modelの学習\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    metrics.reset()\n",
        "\n",
        "    model.train()\n",
        "    with tqdm(total=len(train_dataloader), unit=\"batch\") as pbar:\n",
        "        pbar.set_description(f\"[train] Epoch {epoch+1}/{n_epochs}\")\n",
        "        for image, target in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            image, target = image.to(device), target.to(device)\n",
        "            output = model(image)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            pbar.set_postfix(loss=np.array(train_losses).mean())\n",
        "            pbar.update(1)\n",
        "\n",
        "    model.eval()\n",
        "    with tqdm(total=len(valid_dataloader), unit=\"batch\") as pbar:\n",
        "        pbar.set_description(f\"[valid] Epoch {epoch+1}/{n_epochs}\")\n",
        "        for image, target in valid_dataloader:\n",
        "            image, target = image.to(device), target.to(device)\n",
        "            output = model(image)\n",
        "            loss = loss_fn(output, target)\n",
        "            valid_losses.append(loss.item())\n",
        "            metrics.update(target.argmax(1).cpu().numpy(), output.argmax(1).cpu().numpy())\n",
        "            pbar.set_postfix(loss=np.array(valid_losses).mean(), mIoU=metrics.get_scores())\n",
        "            pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElN-0CGKcJhH"
      },
      "source": [
        "学習が終わったら，FCNによる予測を確認してみましょう．\n",
        "- 左列: 元画像\n",
        "- 中列: 予測\n",
        "- 右列: 正解"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOA33O6fLeUQ"
      },
      "source": [
        "# 予測の可視化\n",
        "plt.figure(figsize=(12, 16))\n",
        "model.eval()\n",
        "for i in range(4):\n",
        "    image, target = valid_dataset[i]\n",
        "    pred = model(image.to(device).unsqueeze(0))\n",
        "    plt.subplot(4, 3, 3*i+1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(image.numpy().transpose(1,2,0));\n",
        "    plt.subplot(4, 3, 3*i+2)\n",
        "    plt.title(\"pred\")\n",
        "    plt.imshow(pred.argmax(1).cpu().numpy()[0]);\n",
        "    plt.subplot(4, 3, 3*i+3)\n",
        "    plt.title(\"gt\")\n",
        "    plt.imshow(target.argmax(0).cpu().numpy());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLjfMMVhPk-d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}