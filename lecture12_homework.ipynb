{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matsu641/DL-practice/blob/main/lecture12_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90u4_gyT2zHC"
      },
      "source": [
        "# 第12回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92iSmEX2zHF"
      },
      "source": [
        "## 課題\n",
        "強化学習に関する確認クイズに回答してください．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 提出方法・評価方法\n",
        "上から回答していくと，提出用csv`submission.csv`が作成されます．\n",
        "**Omnicampusの宿題タブから「第12回 深層強化学習」を選択して**提出してください．\n",
        "\n",
        "- 何回提出しても構いませんが，**採点は締め切り後に一回だけ行われます（即時採点ではありません）**．\n",
        "\n",
        "- リーダーボードはありません\n",
        "\n",
        "- コードの提出はありません"
      ],
      "metadata": {
        "id": "YMOTVpCeCBun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcmwqwJB2zHG"
      },
      "outputs": [],
      "source": [
        "# この部分は修正しないでください\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "NUM_EXAMPLE_QUESTIONS = 2\n",
        "example = np.zeros(NUM_EXAMPLE_QUESTIONS,int)\n",
        "\n",
        "NUM_QUESTIONS = 16\n",
        "myanswer = np.zeros(NUM_QUESTIONS,int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8Ld3gR2zHH"
      },
      "source": [
        "### 回答方法\n",
        "各$\\fbox{　(　)　}$に当てはまるものとして最も適切なものを1つずつ選んでください．\n",
        "\n",
        "**例題**\n",
        "\n",
        "今回の講義は第$\\fbox{　(0)　}$回で，タイトルは$\\fbox{　(1)　}$である．\n",
        "\n",
        "1. 4\n",
        "2. 12\n",
        "3. 深層強化学習\n",
        "4. 深層生成モデル\n",
        "\n",
        "正しいのはそれぞれ，「12」「深層強化学習」なので，以下のように0番目と1番目に2,3を数字で回答します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieHrp_W2zHH"
      },
      "source": [
        "```python\n",
        "example[0] = 2\n",
        "example[1] = 3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONpXb1S52zHH"
      },
      "source": [
        "### 確認クイズ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1.\n",
        "下記の講義で学んだ内容についての文章を読み，空欄を埋めるのに最も適切な語を語群から一つずつ選んでください．\n",
        "\n",
        "以下，\n",
        "* $A$は行動集合\n",
        "* $S$は状態集合\n",
        "* $r: S\\times A\\to \\mathbb{R}$は報酬関数\n",
        "* $P: S\\times A \\to \\Delta(S)$は状態遷移確率\n",
        "* $s_0 \\in S$は初期状態\n",
        "* $\\gamma \\in (0, 1)$は割引率\n",
        "* $\\pi: S \\to \\Delta(A)$は適当な方策\n",
        "* $R$は収益\n",
        "* $\\pi^*\\triangleq \\max_\\pi \\mathbb{E}^\\pi[R]$を最適方策\n",
        "\n",
        "とします．\n",
        "\n",
        "<br/>\n",
        "\n",
        "**マルコフ決定過程と収益**\n",
        "\n",
        "$(A, S, P, r, s_0)$の組のことを$\\fbox{  (0)  }$と呼ぶ（割引率を定義に入れることもある）．\n",
        "\n",
        "特に$P$や$r$が未知な場合に最適方策を求める問題を$\\fbox{  (1)  }$，既知な場合に最適方策を求める問題を$\\fbox{  (2)  }$ と一般に呼称する．\n",
        "\n",
        "また，$S$と$A$が有限集合のマルコフ決定過程を$\\fbox{  (3)  }$と呼ぶ．\n",
        "\n",
        "マルコフ決定過程において，意思決定の長さのことを$\\fbox{  (4)  }$と呼ぶ．\n",
        "\n",
        "特に$\\fbox{  (4)  }$が１の問題をバンディット問題と呼ぶ．\n",
        "\n",
        "また，$\\fbox{  (4)  }$が無限なマルコフ決定過程を無限マルコフ決定過程と呼ぶ．\n",
        "\n",
        "無限マルコフ決定過程では，報酬関数によっては収益が$+\\infty$を取るため，最適方策が区別できなくなる可能性がある．\n",
        "\n",
        "これを回避する方法は様々だが，本講義では割引された期待割引収益を考えることで収益を有限な値に抑えた．\n",
        "\n",
        "特に報酬関数が$\\max_{s, a} |r(s, a)| \\leq 1$を満たす場合，期待割引収益は$\\fbox{  (5)  }$の値でバウンドされる．\n",
        "\n",
        "<br/>\n",
        "\n",
        "**プランニングアルゴリズム**\n",
        "\n",
        "本講義で学んだ価値反復法は，$Q$関数について次の更新を繰り返す．\n",
        "$$\n",
        "Q_{k+1}(s, a)\\leftarrow r(s, a) + \\gamma \\sum_{s'}P(s'|s, a)\\fbox{  (6)  } \\quad s, a \\in S\\times A\n",
        "$$\n",
        "\n",
        "この更新は$\\fbox{　(7)　}$に収束する．\n",
        "\n",
        "また，収束したQ関数は$\\fbox{  (8)  }$を満たす．\n",
        "\n",
        "<br/>\n",
        "\n",
        "**強化学習アルゴリズム**\n",
        "\n",
        "価値反復法において，$\\sum_{s'}P(s'|s, a)$は現実的な問題では計算することが難しい．\n",
        "\n",
        "そこで，Q学習では$\\sum_{s'}P(s'|s, a)$の部分をサンプルで近似し，次の更新を行う：\n",
        "\n",
        "$$\n",
        "Q(s^{(h)}, a^{(h)}) \\leftarrow (1-\\alpha) Q(s^{(h)}, a^{(h)}) + \\alpha \\left(r^{(h)} + \\gamma \\max_{a\\in A} Q(s^{(h+1)}, a))\\right)\n",
        "$$\n",
        "\n",
        "ここで$\\alpha$は学習率である．\n",
        "\n",
        "この更新は$s^{(h)}, a^{(h)}$が全ての状態行動$S\\times A$をカバーし，さらに$\\alpha$が十分小さいならば，$\\fbox{  (7)  }$に収束する．\n",
        "\n",
        "<br/>\n",
        "\n",
        "**深層強化学習アルゴリズム**\n",
        "\n",
        "上で見た価値反復法とQ学習がDeep Q-Network（DQN）の基礎である．\n",
        "\n",
        "DQNは，ニューラルネットワークを用いて$Q$関数を表現する．\n",
        "\n",
        "Q関数をニューラルネットワークを用いて表現する手法はDQN 以前にも提案されていたが，不安定な学習が課題であった．\n",
        "\n",
        "学習の安定化に取り組んだ点がDQN の重要な貢献である．\n",
        "\n",
        "本講義ではDQNの工夫を２つ紹介した．\n",
        "\n",
        "$\\fbox{　(9)　}$は，エージェントが経験した状態・行動・報酬・状態遷移先をデータとして保存する工夫である．\n",
        "\n",
        "DQNは$\\fbox{　(9)　}$に保存されたデータからランダムサンプリングされたミニバッチを用いてQネットワークの更新を行う．\n",
        "\n",
        "$\\fbox{　(10)　}$は，ベルマン誤差計算時の正解ラベルの値に用いるQネットワークのパラメータを固定し，一定周期でこれを更新することで学習を安定化させる工夫である．\n",
        "\n",
        "<br/>\n",
        "\n",
        "【語群】\n",
        "1. ベルマン方程式\n",
        "2. ベルマン期待方程式\n",
        "3. マルコフ決定過程\n",
        "4. テーブルマルコフ決定過程\n",
        "5. 線形マルコフ決定過程\n",
        "6. $\\gamma$\n",
        "7. $(1-\\gamma)^{-1}$\n",
        "8. $Q^\\pi$\n",
        "9. $Q^{\\pi^*}$\n",
        "10. $\\max_{a'\\in A} Q(s', a')$\n",
        "11. $\\sum_{a'\\in A} \\pi(a' \\mid s') Q(s', a')$\n",
        "12. プランニング問題\n",
        "13. 強化学習問題\n",
        "14. experience replay\n",
        "15. REINFORCEアルゴリズム\n",
        "16. ターゲットネットワーク\n",
        "17. 割引率\n",
        "18. 勾配\n",
        "19. ホライゾン"
      ],
      "metadata": {
        "id": "3njPYi-UtKZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myanswer[0] =  # WRITE ME\n",
        "myanswer[1] =  # WRITE ME\n",
        "myanswer[2] =  # WRITE ME\n",
        "myanswer[3] =  # WRITE ME\n",
        "myanswer[4] =  # WRITE ME\n",
        "myanswer[5] =  # WRITE ME\n",
        "myanswer[6] =  # WRITE ME\n",
        "myanswer[7] =  # WRITE ME\n",
        "myanswer[8] =  # WRITE ME\n",
        "myanswer[9] =  # WRITE ME\n",
        "myanswer[10] =  # WRITE ME"
      ],
      "metadata": {
        "id": "gaY4N-XN1z9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip4oZ0Na2zHI"
      },
      "source": [
        "#### Q2．\n",
        "\n",
        "次のアルゴリズムは深層強化学習における代表的な手法です．軽く目を通しておきましょう．\n",
        "\n",
        "**それぞれの手法の説明として最も適切なもの**を以下の選択肢から選んでください．\n",
        "\n",
        "- TRPO $\\fbox{　(11)　}$ https://arxiv.org/abs/1502.05477\n",
        "- DDPG $\\fbox{　(12)　}$ https://arxiv.org/abs/1509.02971\n",
        "- A3C $\\fbox{　(13)　}$ https://arxiv.org/abs/1602.01783\n",
        "- SAC $\\fbox{　(14)　}$ https://arxiv.org/abs/1801.01290\n",
        "- TD3 $\\fbox{　(15)　}$ https://arxiv.org/abs/1802.09477\n",
        "\n",
        "<br/>\n",
        "\n",
        "【選択肢】\n",
        "1. 方策のエントロピー正則化を学習の目的関数に組み込んだアルゴリズム．探索の性能の向上とロバスト性の獲得がエントロピー正則化によって期待される（ちなみに別論文だが，ロバスト性の獲得は理論的に示されている：https://arxiv.org/abs/2101.07012）．\n",
        "2. DDPGにQ関数の過大評価を防ぐための改良を施したアルゴリズム．①複数のQ関数の最小値をTDターゲットに用いるclipped double Q-learning，②価値の勾配の分散を低減させるために，方策の更新頻度をQ関数よりも減らす，③価値関数にランダムノイズを加えることで平滑化を行う正則化項の導入，を提案している．\n",
        "3. 複数のactorが並列的に学習しながら，criticが価値関数を学習するactor-criticアルゴリズム．グローバルなパラメータをもち，各ネットワークのパラメータがそのグローバルなパラメータで非同期的に更新される．\n",
        "4. Actor-criticの枠組みを用いて決定的な方策を学習することで，DQNを連続値行動空間に拡張した手法．\n",
        "5. 学習の安定性を高めるために，1回のパラメータの更新で方策が大きく変化するのを防ぐアルゴリズム．更新前と更新後の方策の間にKLダイバージェンスによる制約をかける．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xWtIRso2zHI"
      },
      "outputs": [],
      "source": [
        "myanswer[11] =  # WRITE ME\n",
        "myanswer[12] =  # WRITE ME\n",
        "myanswer[13] =  # WRITE ME\n",
        "myanswer[14] =  # WRITE ME\n",
        "myanswer[15] =  # WRITE ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Q1aoGK2zHJ"
      },
      "outputs": [],
      "source": [
        "# ドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 作業ディレクトリを指定\n",
        "work_dir = 'drive/MyDrive/Colab Notebooks/DLBasics2025_colab'\n",
        "\n",
        "if not np.any(myanswer==0):\n",
        "    submission = pd.Series(myanswer, name='label')\n",
        "    submission.to_csv(work_dir + '/Lecture12/submission.csv', header=True, index_label='id')\n",
        "else:\n",
        "    print('Error: please answer all of the questions')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}